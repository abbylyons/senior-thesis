% TEMPLATE for Usenix papers, specifically to meet requirements of
%  USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
%   written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
%   Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
%   both the above pioneers
% use at your own risk.  Complaints to /dev/null.
% make it two column with no page numbering, default is 10 point

% Munged by Fred Douglis <douglis@research.att.com> 10/97 to separate
% the .sty file from the LaTeX source template, so that people can
% more easily include the .sty file into an existing document.  Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file. 

% Note that since 2010, USENIX does not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the 
% usepackage command, below.

\documentclass[letterpaper,12pt]{article}
\usepackage{usenix,epsfig,endnotes}
\usepackage{courier}
\usepackage{tikz}
\usetikzlibrary{arrows,automata, positioning}
\begin{document}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\bf \LARGE Generating Tests to Verify Machine Descriptions}

\author{
{\rm Abigail Lyons}\\
{\rm \today}
}
\maketitle
\thispagestyle{empty}

\vspace*{2\baselineskip}
{\centering
  \bf{Abstract}\par
}
With the ending of Moore's Law, people are increasingly turning to other ways to leverage performance. x86 has been an industry standard for decades, but in recent years RISC ISAs like ARM and specialized hardware like FPGAs and GPUs are becoming widely used. Because of this, the already labor-intensive task of porting an operating system to different hardware needs a better solution. One possible approach to making porting easier is code synthesis through machine-level and semantic descriptions of hardware. Therefore, it is crucial that machine descriptions accurately reflect the hardware that they attempt to describe. Given a machine description, we generate a suite of tests that, when executed on the target platform, verifies that the description accurately reflects the platform's expected behavior.

\pagebreak

\tableofcontents

\pagebreak

\section{Introduction}

The introduction needs to tell the story of:

1. A richer description of the problem than is in the abstract (the story line is in the abstract, but it needs to be fleshed out).

2. The story goes something like:

Hardware is proliferating

Hardware platforms need operating systems

Operating systems are challenging and tedious to port

If we could describe hardware platforms in a sufficiently rich way we could synthesize them

There are lots of different ways to describe HW

We need a way that satisfies these requirements

However, writing those descriptions is challenging — better than having to port every piece of MD software, but challenging nonetheless.

So, if this is all going to work, we need a way to make sure those descriptions are correct.

How might you do that/

Well, if you could generate tests to run on the real hardware and could figure out a way to determine if the results of that execution were what the description said it was, that would be awesome.

So, we’re going to that awesome thing.

\subsection{Contributions}

This paper presents a testing methodology for machine descriptions that generates the test suite from scratch. First, the paper describes a design and implementation of a program that can run equivalent instructions on a virtual test machine and a Cassiopeia interpreter and directly compare the resulting states. Second, it details a method of generating randomized tests from scratch, pruning the random tests for validity, and selecting important corner cases to catch common errors. And lastly, it shows that this testing method reveals programmer errors in a Cassiopeia machine description of an ARM processor.

The rest of this thesis is structured as follows:

In Chapter 2, we provide some background on the key infrastructure pieces on which this work depends

In Chapter 3, we describe the test infrastructure we built

In Chapter 4, we demonstrate the infrastructure on the ARM processor

In Chapter 5 we conclude

\section{Background}
\subsection{Machine Descriptions}

A machine description language is any language that describes hardware. Hardware description languages (HDLs), such as Verilog, describe properties of machines and are commonly used when designing digital circuits, and as such they fit this general definition. However, in this paper I will instead focus on semantic machine description languages, which describe at a high level how each instruction changes the state of a machine \cite{Design}. A semantic machine description language consists of two main components. The first is a state definition, which defines what types are available, the number of registers and their types, flag bits, the amount of memory available, and possibly even more information describing the machine's state. The second component is the instruction definitions, which define instructions like shift right, add, etc. in terms of how the machine's state changes. When testing a completely accurate machine description, the values in the defined state should always match the actual state of the machine it's describing.

One example of a buggy machine description is the following definition of add. Ignoring flag bits and overflow behavior, this is supposed to add the contents of registers \texttt{r1} and \texttt{r2} and store the result in \texttt{r2}:\\

\texttt{add r1, r2:}

\texttt{r2 $\leftarrow$ r1}\\

This obviously doesn't produce the intended result, and this bug can be easily found by running one add instruction in isolation. Compare this with the following description for \texttt{add}:\\

\texttt{add r1, r2:}

\texttt{r3 $\leftarrow$ r1 + r2}

\texttt{r2 $\leftarrow$ r3}\\

This is less obviously wrong, because running one add instruction in isolation will not show any bugs just from examining the result. To find it, one would need to subsequently use r3 in another instruction and get an incorrect result, and then attempt to figure out when the original value of r3 was incorrectly replaced. In a machine where the state can be examined, the examiner would need to look at least at the values of all registers, including ones that were not explicitly used, in order to find this bug.

\subsection{Cassiopeia}

% FIX ME - figure out how to cite Cassiopeia documentation?
Cassiopeia is a new semantic machine description language, where a description consists of an enumeration of a machine's state and an implementation of operations in terms of the machine's state. For any given processor, much of this information can be found in its documentation to varying degrees of accuracy. Someone who wants to use Cassiopeia would then need to manually translate the documentation into Cassiopeia code.

Cassiopeia has a few primitive types, including a length n bitvector, logical integer, logical boolean, pointer, and others. All other types used in the machine description must be defined by the user in terms of already existing types. Following are some examples of these type definitions; the first line creates a type \texttt{word} that is 32 bits long, while the second line creates a type \texttt{register} that is a pointer to a word, or a length-32 bitvector.\\

\texttt{type word = 32 bit}

\texttt{type register = word loc}\\

To describe a machine state, Cassiopeia has a \texttt{letstate} which names variables of given types, along with their initialized values, as part of the state. These statements can be used to represent condition bits like N (indicating a negative result on an ARM processor), general registers like R0 (which stores return values on an ARM processor), and memory, represented here as a single word to shrink the size of the state space.\\

\texttt{letstate N:1 bit loc = 0x0}

\texttt{letstate V:1 bit loc = 0x0}

\texttt{letstate R0:register = 0x00000000}

\texttt{letstate R1:register = 0x00000000}

\texttt{letstate MEM:word loc = 0x0}\\

Cassiopeia describes how instructions change the machine's state using a \texttt{defop} for each instruction.  Here is an example of the logical shift left operation in ARM; note that \texttt{bv\_to\_uint(bv)} is a built-in Cassiopeia function that translates a bitvector into an unsigned integer.\\
        
\texttt{defop LSL Rd:register Rn:register sh:halfword \{}

\texttt{\ sem = [}

\texttt{\ \ let sh\_int:int = arm32\_16touint(sh) in}

\texttt{\ \ let dRn:word = *Rn in}

\texttt{\ \ let shgtz:bool = sh\_int > 0 in}

\texttt{\ \ if shgtz then}

\texttt{\ \ \ let shRn:word = dRn << sh\_int in}

\texttt{\ \ \ let iszero:bool = shRn == 0 in}

\texttt{\ \ \ *N <- shRn[31:32];}

\texttt{\ \ \ *Z <- iszero ? 0x1 : 0x0;}

\texttt{\ \ \ *Rd <- shRn}

\texttt{\ \ else}

\texttt{\ \ \ let iszero:bool = dRn == 0 in}

\texttt{\ \ \ *N <- dRn[31:32];}

\texttt{\ \ \ *Z <- iszero ? 0x1 : 0x0;}

\texttt{\ \ \ *Rd <- dRn}

\texttt{\ ]}

\texttt{\}}\\

The first line contains the name of the operation, followed by its inputs and their types. The second line declares the beginning of the operation's semantics. Temporary variables are defined using \texttt{let...in} statements. Assignment of state variables requires dereferencing them using \texttt{*} and then assigning them using \texttt{<-}. Logical operations like +, $>>$, etc. are all built into the language, allowing the programmer to focus on the changes to the machine state instead of having to completely re-implement an add instruction.

Ultimately, what the above example aims to do is check whether the given shift value is greater than zero. If it is, it shifts the value in \texttt{Rn}, sets the \texttt{N} flag to the highest bit, and sets \texttt{Rd} to the shifted value; otherwise, it sets the \texttt{N} flag to the highest bit of \texttt{Rn} and sets \texttt{Rd} to \texttt{Rn}. In both cases, the \texttt{Z} flag is set to 1 if the result is 0, and set to 0 otherwise. Note that, compared to this brief statement of pseudocode, the actual Cassiopeia code appears to be much more verbose. This is because Cassiopeia does not yet allow multi-step statements, such as \texttt{if sh\_int > 0 then}, where an implicit step calculates the intermediate anonymous boolean \texttt{sh\_int > 0}.

In summary, a Cassiopeia file consists of type definitions, followed by state definitions and then instruction definitions.

There are two useful tools for writing Cassiopeia code; one is a parser that builds an abstract syntax tree out of a machine description, while the other is an interpreter that runs a given Cassiopeia program using a machine description. The machine description testing program delineated in this paper uses the interpreter to run instructions and output the expected state of the machine. The Cassiopeia program that the interpreter uses consists of statements written in the same form as the following example:\\

\texttt{(ADD R0 R0 0x0000abcd)}

\texttt{(ADD R1 R0 0x00001111)}\\

Assume that all registers and other components of state are initially set to \texttt{0x00000000} and an \texttt{ADD} function is correctly defined. If the above program interprets successfully, the interpreter will output that \texttt{R0 = 0x0000abcd}, \texttt{R1 = 0x0000bcde}, and all other components of state are equal to \texttt{0x00000000}.

\subsection{ARM v6 Architecture}

The ARM has 31 general-purpose registers, all of which are 32 bits wide, the first 16 of which are the user mode registers available to all unprivileged code. The first four registers (R0-R3) hold function call arguments, while R4-R8 do not have any designated purpose. R13 is typically the stack pointer, R14 is the link register which holds the address of the next subroutine call instruction, and R15 is the program counter which is two instructions after the instruction being executed. The ARM also has a current program status register (CPSR), which contains information such as endianness, interrupt masks, and other condition flags. Most importantly for this application, in the highest 4 bits it holds condition code flags representing a \textbf{N}egative, \textbf{Z}ero, \textbf{C}arry, and o\textbf{V}erflow result\cite{ARM}. For this purpose, I model a partial state consisting of the first 9 registers, the four condition code flags, and one 32-bit piece of memory. This restricts which instructions I can model, making instructions like branch impossible to represent.

Because ARM is a Reduced Instruction Set Computer (RISC), it has fixed-length instruction fields, making it ideal for modeling in Cassiopeia. Most ARM instructions are capable of setting condition flags by ending the instruction name with an s (i.e. \texttt{adds} instead of \texttt{add}); noteworthy exceptions are the branch instruction and its variants, which never set condition flags, and the compare instruction and its variants, which always set all of the condition flags. As such, modeling the condition flags in Cassiopeia is an important to getting the semantic instruction definitions exactly right.

To run one test of a machine description of ARM, the same instruction must be run with the same initial state in both the Cassiopeia interpreter and on an ARM processor. Only if their final states are the same does the test pass.

\subsection{Simulating ARM}

The program explained in this paper was not run on the physical ARM processor that the Cassiopeia machine description is written for. Rather, it was run on a fixed virtual platform (FVP) that intends to model the underlying state of an ARM v7 processor and memory\cite{FVP}. This differs from an emulator, which attempts to mimic the outward behavior of its target without necessarily mimicking the underlying state. For this application, an emulator that is not a simulator is not sufficient, since the tests rely heavily on extracting that underlying state.

The ARM FVP can easily be booted on an x86 machine running Ubuntu 17.10, and it can share the host's filesystem, making communication between programs on the FVP and the host much simpler. For purposes of this paper, I will continue to treat the ARM FVP as though it accurately represents a real ARM v7 processor, even if it has not been proven to be completely correct.

\subsection{Test Generation Challenges}

Because testing each instruction definition in isolation does not guarantee a correct machine description, we must introduce numerous more instructions to determine that each instruction actually being tested is correct. This leads to the state explosion problem that many testing problems face: as the number of instructions in a program, or the number of definitions in a machine description, increases, the number of tests required to guarantee correctness grows exponentially\cite{Klee}.

However, even when testing instructions just in isolation, the state space is still too big to test exhaustively. Suppose there is a very simple Turing-complete machine description for a 32-bit processor that just consists of load and store. For either of those instructions, the first register can contain any value between 0 and $2^{32} - 1$. The second register can contain any valid address. The third value, an immediate representing the offset from the address stored in the second register, can contain any number of values between the address and the end of valid memory\cite{ARM}. Assuming that the size of user-accessible memory is around 4 MB, this means that exhaustively testing the load instruction for two fixed registers alone would require $2^{32} * \sum_{x=0}^{4M-1}x(4M-x) = 2^{32} * 1.067*10^{19} \approx 4.5 * 10^{28}$ tests.

To give an idea of what this means to someone trying to test their machine description, running 10000 tests on a computer with a 2.7 GHz processor and 8 GB RAM takes approximately 2 minutes and 30 seconds; running all the tests for the load instruction for those two predefined registers would then take $2.5 * 4.5 * 10^{24} \approx 1.1 * 10^25$ minutes, or $2.1 * 10^{19}$ years! Even though exhaustive testing in isolation is just as hopeless as exhaustively testing combinations of instructions, I opted to start with a new state for each instruction rather than track the changes in a single state across multiple instructions.

\section{Related Work}

\subsection{Machine Descriptions}

Machine descriptions based on register transfer lists (RTLs) have been frequently used for building retargetable compilers, but RTL languages are typically verbose and difficult for a human reader to understand. $\lambda$-RTL was developed to make machine descriptions easier to use by adding some human-readable shortcuts, such as allowing implicit fetches and assignment to bitvector slices. Ultimately, the goal of this is that machine descriptions will be more widely used and reused to build other retargetable tools, not just compilers but also simulators, assemblers, linkers, and debuggers\cite{Mdesc}.

One interesting application of semantic machine description languages is verification of high-performance cryptographic assembly code. The Vale language was developed for this purpose. A programmer first describes the syntax and semantics of their architecture in Dafny, and then uses Vale to describe how the architecture's state is manipulated. The Vale tool generates an abstract syntax tree (AST), and builds proofs that are verified using a SMT solver\cite{Vale}. Cassiopeia has some similarities to Vale, since it primarily describes how state is manipulated and also ultimately generates an AST.

Another application of machine description languages is testing individual components of a compiler, such as the architecture specifications used by code generators. GCC has its own RTL specification which describes the semantics of each instruction, but it cannot be assumed to be correct. Whenever a semantic difference is found in the testing process, it must be manually verified that the RTL semantics match the crresponding assembly instruction\cite{Cgo}.

Notably, all these applications assume that the programmer provides an accurate description of their machine or rely on manual verification.

\subsection{Test Generation}

There have been many attempts to address the path-explosion problem of testing with well-defined heuristics of generating tests. However, most of these involve using existing source code as tests, and using a combination of path exploration and pruning heuristics to figure out which tests are most effective.

One of the classic methods of generating tests is symbolic execution, which is designed to test a given program. It replaces the program's inputs with symbolic variables that have no constraints. It then gradually constrains the inputs more and more as it attempts to do two things: hit every possible line of executable code, and check each line against all possible input values. To address the path-explosion problem, it uses search heuristics to select which path to choose at each instruction step, taking into account the minimum distance to an uncovered instruction, among other things. Because the highest-coverage tests are run first, KLEE can then be run with a time limit and still provide confidence that many of the test cases have been covered \cite{Klee}. Symbolic execution could be used to test machine descriptions, but the test program would need to be written ahead of time. Even if KLEE could run to completion on the test program, the test program itself cannot be guaranteed to cover all possible use cases.

One redeeming factor of KLEE is that it has since been extended to do symbolic execution at the binary level, and various other optimizations can be made for low-level applications. This is particularly relevant for systems applications; for example, if testing that a pagetable works as expected, one would only need to explore a single pagetable location and test a few random values in the table \cite{Path}. This same reasoning can be used to remove redundant tests by reducing the number of times the same instruction is called with different operands.

A different approach to testing is fuzzing. First, the target's state is first filled with random data. Next, operation codes are chosen and combined with predefined operand values. The resulting tests are then pruned for redundancy and validity. The actual and expected states are compared at the end of execution of each test case. This test generation is similar to the one used in this paper, though I used a much smaller sample of the machine state. The biggest difference is the objective; others have used fuzzing to discover whether an emulator behaves like the machine it claims to emulate\cite{Emu}, find differences in disassemblers\cite{Nvd}, or find instances in which hardware behavior does not match its online documentation. Many of the solutions to emulator, simulator, and hardware testing can also be applied to machine description testing.

One technique that has already been applied to machine description testing is assembly-disassembly. First, a sequence of instructions is encoded in an assembly file and a binary file. An independent assembler is applied to the assembly file to produce a second binary file, or an independent disassembler is applied to the binary to produce a second assembly file. When the resulting two assembly or binary files are compared, it is expected that they will be the same; a difference suggests an error in the machine description. One drawback of this approach is that assemblers and disassemblers frequently have bugs, and other research has been directed toward finding those bugs by comparing the output of disassemblers\cite{Nvd}, so it is not immediately clear whether a found bug originates from the machine description or the assembler or disassembler. This also raises the question of which instructions, inputs, and values should be tested. In the same paper that introduced this approach, test generation involved building a tree out of each instruction's possible inputs, selecting the branch that covers the most untested inputs, and then using heuristics to decide on the values of those inputs\cite{Checker}. This works well for RISC descriptions, but in more complex machines that allow a large number of inputs in their instructions, using a similar method would require implementing and running a complicated integer linear program solver to achieve similar results.

\section{Design and Implementation}

\subsection{Problem Statement}

My solution to testing machine descriptions is a program which generates initial states and test instructions, initializes the states and runs the instructions on both the Cassiopeia interpreter and the test machine using inline assembly, and compares the resulting states. The resulting states must match completely for the test to pass. Figure 1 is an illustration of this flow of information for a single test. $S, t$ represent an initial state and test instruction from the test generator. $\alpha$ represents a function that translates the initial state into its equivalent on the ARM processor, $\beta$ represents a function that translates the initial state into its equivalent in Cassiopeia, and their inverses undo those functions to get a state in the same form as the initial state. $asm$ and $casp$ are functions that run the given test on translated states to get new translated states.

\begin{figure}
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
	semithick]
	\tikzstyle{every state}=[fill=none,draw=black,text=black,minimum width=1.8cm]
	
	\node[] (s) {start};
	\node[state] (0) [right=1cm of s] {$S, t$};
	\node[state] (1) [above right of=0] {$\alpha(S), t$}; 
	\node[state] (2) [below right of=0] {$\beta(S), t$}; 
	\node[state] (3) [right of=1] {$\alpha(S)'$}; 
	\node[state] (4) [right of=2] {$\beta(S)'$}; 
	\node[state] (5) [below right of=3] {$S'_a \stackrel{?}{=} S'_b$}; 
	\node[] (r) [right=1cm of 5] {result};
	
	\path
	(s) edge node {} (0)
	(0) edge node [left] {$\alpha$} (1)
	(0) edge node [left] {$\beta$} (2)
	(1) edge node [above] {$asm$} (3)
	(2) edge node [below] {$casp$} (4)
	(3) edge node [left] {$\alpha^{-1}$} (5)
	(4) edge node [left] {$\beta^{-1}$} (5)
	(5) edge node {} (r)
	;
\end{tikzpicture}
\caption{logical flow of information through a single test.} \label{fig:M1}
\end{figure}

The diagram shows that there could be a few different sources of failure should $S'_a$ and $S'_b$ not match. One could be that $\alpha$ is not accurately translating the state, and another could be that $\alpha^{-1}$ is not a perfect inverse of $\alpha$; $\beta$ is also subject to these possible errors. The one that's interesting for purposes of this paper, however, is that $asm$ and $casp$ do not do equivalent operations on the translated states, and the goal of the generated $S$ and $t$ is to uncover these kinds of errors.

\subsection{Implementation}

\begin{figure}
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
	semithick]
	\tikzstyle{every state}=[fill=none,draw=black,text=black,minimum width=1.8cm]
	
	\node[] (s) {inputs: number of tests, type of test generator, casp file};
	
	\node[state] (0) [below=1cm of s] {state generator};

	\node[] (r) [below=1cm of 0] {result};
	
	\path
	(s) edge node {} (0)
	(0) edge node [left] {$\alpha$} (0)
	(0) edge node {} (r)
	;
\end{tikzpicture}
\caption{procedure for generating and running one test} \label{fig:M2}
\end{figure}

\subsubsection{State Implementation}
% FIX ME - cite synchronized state compar

\subsubsection{Test Implementation}

\section{Evaluation}

\subsection{Bug Analysis}

\subsection{Things I Haven't Implemented or Did Poorly (do I need this?)}

\section{Conclusion}

\section{Acknowledgements}
First and foremost, I would like to express my deepest gratitude to my advisors, Professors Margo Seltzer and Stephen Chong, for their patient explanations of topics that I have no prior experience in, not to mention their immensely helpful feedback on my ideas, code, and writing. I would also like to thank everyone at Harvard working on the PRINCESS project for their support and camaraderie, and building the tools necessary for me to complete this research. And lastly, I'd like to personally thank Timothy for four remarkable years of friendship, and my mother, father, and brother for their lifelong support of my studies.

{\footnotesize \bibliographystyle{acm}
\bibliography{sample}}


%\theendnotes

\end{document}
