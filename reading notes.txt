-----Automatic Checking of Instruction Specifications--------------------------
checker.pdf
The goal is to automate retargeting applications that use machine code by (1) describing machine instruction sets in a high-level specification language, and (2) generating the code for encoding and decoding instructions. Specifications must be consistent both internally and externally. Internal checking looks for impossible insturction specifications, like specifying two values for one bit or not specifying values for all bits. External checking makes sure the specification is accurate.

My reactions:
1) While SLED doesn't require specifying operations at the bit level, it still looks nontrivial to figure out what all of those fields mean. 
2) In the "Checking for Correctness" section, apparently we still need to choose a sequence of instructions to test.

This paper describes how they used SLED (Specification Language for Encoding and Decoding), which is a language for describing an instruction set. This could be useful, but the paper doesn't tell us which instructions to test, other than a few crude examples.


-----KLEE: Unassisted and Automatic Generation of High-Coverage Tests----------
klee.pdf
KLEE is a symbolic execution tool that generates tests to achieve high coverage on a diverse set of complex and environmentally-intensive programs. It replaces a program's inputs with symbolic variables whose values are unconstrained, and constrains them as it tries to (1) hit every line of executable code, and (2) check each line against all possible input values. It's implemented in Low Level Virtual Machine assembly, which uses a RISC-like instruction set with an infinite number of registers, and functions as both an operating system and an interpreter. 

My reactions:
1) KLEE makes performance improvements (such as query optimization), but ultimately this doesn't solve the problem that the number of tests grows exponentially with the size of the program.
2) There's some interesting stuff about debugging systems code with KLEE, especially with filesystem and failing system calls. I don't see it being relevant, however.

KLEE uses search heuristics to select which process to run at each instruction step, which take into account the min distance to an uncovered instruction and whether the process recently covered new code. LLVM itself could be useful, because KLEE has accurate bit-level modeling of legal C operations. If I were to use symbolic execution to evaluate programmed tests, I would still need some way to generate that code in the first place, which this paper does not answer. However, I could write something that generates tests indiscriminately and have a symbolic execution tool decide which tests would test the most new things.


-----Vale: Verifying High-Performance Cryptographic Assembly Code--------------
vale2017.pdf
Vale is a programming language and tool which transforms annotated assembly language into an abstract syntax tree with proofs about the AST that are verified using an SMT solver. Machine syntax and semantics are written in Dafny, which is some other Microsoft framework, and then Vale is used to manipulate the syntax and semantics. This paper also contributes a series of case studies applying Vale to cryptographic algorithms on x86, x64, and ARM platforms.

My reactions:
1) There are so many cool security applications of this, like taint analysis and cryptographic library testing! Figure 9 has an example of using the .trace method to find leaks. Too bad I'm not researching this :')
2) I'm a bit surprised by how low-level Vale procedures are--but I suppose it makes sense. For example, there's this procedure in the example called Add3ToR7() which calls AddOne(r7) three times. Pretty simple.

This paper has good examples of using Dafny to define simplified ARM, as well as examples of using Vale to verify that various programs work, with the added bonus that Vale does not need to know about particular assembly language architectures. 

One general observation I've made: So far, it seems that everything I've read helps with (1) the machine description part, and/or (2) testing a low-level program to make sure it works. But there's a part missing in between that tells us what sequences of instructions would be useful to test. It's not as simple as writing a program and using a verifier to check it.


-----Testing CPU Emulators-----------------------------------------------------
testing cpu emu.pdf
This paper presents specially crafted test cases and uses fuzzing to see whether an emulated CPU is correct. Each test case is a configuration of the environment, and then the two environments are compared at the end of execution of each test case. It defines state-transition functions where each state contains a program counter, register state, memory state, and exception state before and after executing the current instruction; the PC and the emulator must have the same state at all times for the test to pass. Fuzzing generates synthetic states and runs the instruction on both the PC and the emulator.

My reactions:
1) I wish some of these papers did something smarter than random test generation with really simple heuristics. In this case, for random test-case generation it maps a file filled with random data until the entire user address space is filled (but at least the mapping is lazy).
2) CPU-assisted test-case generation generates code algorithmically, but still generates data randomly. I'm still skeptical that this is the best way of coming up with tests. It says "for each opcode we generate test-cases by combining the opcodes with some predefined operand values", but it doesn't really say how to come up with those predefined values.
3) There seems to be some sort of tree which relies on the fact that each instruction has a unique prefix, followed by some bytes which represent operands, and it is supposedly pretty easy to prune that tree.

It seems that until now, we've been missing a method of coming up with useful tests; fuzzing could be it. It also describes how to set up the test execution environments.


-----Path-Exploration Lifting: Hi-Fi Tests for Lo-Fi Emulators-----------------
path exploration lifting.pdf
This paper explores a high-fidelity emulator with symbolic execution, then lifts those test cases to test a lower-fidelity emulator--though this technique can be further validated by running the tests on real hardware. It uses FuzzBALL, kind of like KLEE but a lightweight engine for symbolic execution designed for binary-level program representation. To generate test programs, it writes a baseline state initializer which generates a small environment necessary for running all tests, then builds programs by finding a gadget for each component of the test state and instantiating additional gadgets to correct the side effects until there are no more outstanding side effects.

My reactions:
1) There seem to be a lot of highly technical/systems-y details in here. This isn't necessarily a bad thing, though I decided to skip over a lot of it when reading this paper.
2) FuzzBALL is described as an interpreter for machine instructions that does online symbolic execution, which might be exactly what I'm looking for. Constraints for online exploration include (1) feasibility, and (2) the path not being explored before. Read STP and Z3 for more implementation details about deision making.
3) It seems to operate on very low memory--it does stuff basically randomly, opting to repeat paths if necessary instead of storing multiple states. However, it also stores precise summaries of common code regions to avoid path explosion.
4) Seems like they want to use a large state space (marking a lot of the state as symbolic), while avoiding large numbers of executions that are effectively identical. For example, you only have to explore one pagetable location, and you can just test a few random values in the pagetable. Makes a lot of sense. (And to initialize large tables, parts are concretely initialized to point to things, but flags are symbolic.)

Unlike the previous paper, this one uses symbolic execution to generate tests--and it's at the binary-level! It's also very particular about how it constrains the state space. Seems like I could lift a few of their techniques.

However, none of these papers generate tests from scratch--they're all part of a previously existing program.


-----Mechanizing Assembler Hazard Checks With Machine Descriptions-------------
dholland-qual.pdf (section 8)
This related work section is relevant to me mainly because of the machine descriptions part. Register Transfer Lists encode machine instruction semantics as transfer functions on concrete processor state; CISL is a different description language that describes both instruction encodings and semantics, and can be higher level than RTLs; and there are other languages like Wagstaff that add semantic functionality to lower-level description languages like ArchC. It then goes on to describe SLED usage, as well as its limitations; for example, it requires specifying the positions of fields, and doesn't allow for identical assembly-level instructions that parse to different machine-level encodings.

My reactions:
1) Go David!
2) Seems like SLED is not as easy to use as I had hoped, but it may still be the best option. (David clearly thought so, at least for his project.)

This is useful because of (1) the machine description classifications, and (2) the application of SLED descriptions.


-----Statically-Directed Dynamic Automated Test Generation---------------------
static dynamic tests.pdf
The technique presented folds multiple traces together to build a Visibly Pushdown Automaton to represent the global control-flow of a binary program. It then uses static analysis to find potential vulnerabilities. Finally, it assigns weights to VPA edges and uses symbolic execution to direct exploration to the targeted potential vulnerabilities.

My reactions:
1) It handles misaligned reads and writes, but what if we want to actually catch those as errors?
2) It's pretty awesome that they provided results for each stage separately--most other papers I've read only find results for the method as a whole.
3) Ultimately, this approach is still pretty brute-force, but it's unclear what the tradeoff between precision of static analysis and computed guidance information is.

The intro talks about state-space explosion which I might also want to do. The static analysis part finds various errors, like misaligned reads and writes and out-of-bounds access. There isn't really any analysis, static or dynamic, that I can use on a program because I'll be generating tests from scratch.


-----Machine Description Design------------------------------------------------
machine description design.pdf
This paper presents a confederated approach to machine-description languages, and the goals and principles that have governed the design and implementation of them. It differentiates between representations of machine instructions (SLED) and semantics of machine instructions (lambda-RTL), reinforces the need for abstractions in retargeting applications, and explains why behavior-based languages like Verilog are too low-level. Some design principles include reusable descriptions, meaningful partial descriptions, and reliable descriptions (the author is confident that they wrote what they intended).

My reactions:
1) "Machine-level software tools could be retargeted much more easily if the machine-dependent parts were generated from machine descriptions, rather than written by hand." This is literally the exact sentence I was looking for.
2) Apparently VHDL and Verilog describe properties of machines because they're too low-level and focus on behavior instead of semantics (and only chip designers care about that, not people who read architecture manuals). Sounds like I should stay away from those.
3) Seems like there's something called CSDL (Computer Systems Description Language) that supports both reusable and application-dependent parts, which I should definitely look into. (Bailey and Davidson)
4) SLED uses bit vectors (called tokens) to represent whole RISC instructions, and patterns to describe binary representations (which include a constructor's name, its operands, and a pattern in which they appear as free variables). 
5) lambda-RTL describes how each instruction changes the state of the machine. The examples vaguely resemble what Ming was showing me yesterday.

This provides some much-needed background on machine descriptions. It focuses on languages that can be used in retargeting machine-level software tools which is exactly the application I care about, and it says that the only useful languages are SLED and lambda-RTL, which helps me eliminate other languages. 


-----Machine Descriptions to Build Tools for Embedded Systems------------------
mdesc.ps
This paper uses machine-independent semantics to describe RTLs (register transfer lists)

My reactions

Why this paper is useful


-----Missing papers------------------------------------------------------------
"Automated ISA branch coverage analysis and test case generation for retargetable instruction set simulators"
"Directed Test Case Generation for x86 Instruction Decoding"
"Rapid prototyping and compact testing of CPU emulators"
